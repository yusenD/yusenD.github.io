---
layout:     post
title:      "机器学习从入门到放弃"
subtitle:   "从一个简单的房价预测模型出发，了解TensorFlow的用法"
date:       2020-03-15 12:00:00
author:     "Luyuan"
header-img: "img/post-bg-unix-linux.jpg"
catalog: true
tags:
    - 机器学习
    - TensorFlow
---


## 前置知识

### 监督学习（Supervised Learning）

监督学习是机器学习方法中的一种，指从训练数据（输入和预期输出）中学到一个模型（函数），并根据模型可以推断新实例的方法。

函数的输出通常为一个连续值（回归分析）或类别标签（分类）。

监督学习典型算法：

* 线性回归（Linear Regression）
* 逻辑回归（Logistic Regression）
* 决策树（Decision Tree）
* 随机森林（Random Forest）
* 最临近算法（k-NN）
* 朴素贝叶斯（Naive Bayes）
* 支持向量机（SVM）
* 感知器（Perveptron）
* 深度神经网络（DNN）

![image-20200312114855594](/Users/yuhao/Library/Application Support/typora-user-images/image-20200312114855594.png)

### 线性回归

统计学中，线性回归是利用称为**线性回归方程**的最小二程函数对**一个**或**多个**自变量和因变量之间关系进行建模的一种回归。这种函数是一个或多个成为回归系数的模型参数的线性组合。

#### 单变量线性回归

理想函数：`y = b + wx`

假设函数：这里为了方便用矩阵，添一个X0![image-20200312115439197](/Users/yuhao/Library/Application Support/typora-user-images/image-20200312115439197.png)

损失值（误差）：![image-20200312115453917](/Users/yuhao/Library/Application Support/typora-user-images/image-20200312115453917.png)

为了从一组样本中估计最合适的值，通常采用最小二乘法，其优化目标位最小化残差平方和：

![image-20200312115728329](/Users/yuhao/Library/Application Support/typora-user-images/image-20200312115728329.png)

为了消除求导产生的系数，优化成：

![image-20200312115815783](/Users/yuhao/Library/Application Support/typora-user-images/image-20200312115815783.png)

![image-20200312170134292](/Users/yuhao/Library/Application Support/typora-user-images/image-20200312170134292.png)



> TIPS：
>
> 这里第一次看的时候感觉公式不是特别容易看懂，这里的x实际上是矩阵(x0,x1)，theta也是矩阵(theta0,theta1)。
>
> 所以这个j的下标就很容易理解了，然后再多阶函数求导f(g(x))'=f'(x)*g'(x)

#### 多变量线性回归

这里和上面类似，不同的是矩阵更大一些，输入的训练数据也会多一个x2(i). 

![image-20200312170803063](/Users/yuhao/Library/Application Support/typora-user-images/image-20200312170803063.png)

![image-20200312172116850](/Users/yuhao/Library/Application Support/typora-user-images/image-20200312172116850.png)

> TIPS：
>
> 这里的多元开始也没看懂，其实按照单变量一样的方法展开，发现差别之处就是变成了用矩阵表示而已。
> $$
> (x1-y1)^2 + (x2-y2)^2 + ... + (xn-yn)^2 = (X-Y)^T * (X-Y)
> $$



## TensorBoard可视化工具

 数据处理过程中，数据集分布情况

模型设计过程中，数据流图是否正确

模型训练过程中，模型参数和超参数变化趋势

模型测试过程中，准确率和召回率



### 使用流程

可视化数据是数据流图和张量，他们需要在会话中加载或者执行操作后才能获取。然后用户需要使用**FileWriter**实例将这些数据写入事件文件。最后启动TensorBoard程序，加载事件文件中的序列化数据，从而可以在各个面板中展示对应的可视化对象。

![image-20200315174514189](/Users/yuhao/Library/Application Support/typora-user-images/image-20200315174514189.png)

### tf.sunmmary

FileWriter实例和汇总操作（Summary Ops）都属于tf.summary模块。主要功能是获取和输出模型相关的序列化数据，它贯通TensorBoard的整个使用流程。

tf.summary模块核心部分由一组汇总操作和FileWriter、Summary、Event3个类组成。

## 房价模型预测实战

```python
import pandas as pd
import numpy as np

# 标准化函数
def normalize_feature(df):
    return df.apply(lambda column: (column - column.mean()) / column.std())

# 读入并标准化数据
df = normalize_feature(pd.read_csv('data1.csv',
                                   names=['square', 'bedrooms', 'price']))

ones = pd.DataFrame({'ones': np.ones(len(df))})# ones是n行1列的数据框，表示x0恒为1
df = pd.concat([ones, df], axis=1)  # 根据列合并数据

取出X和Y
X_data = np.array(df[df.columns[0:3]])
y_data = np.array(df[df.columns[-1]]).reshape(len(df), 1)

-------------------------------------
# 创建数据流图
import tensorflow as tf
tf.compat.v1.disable_eager_execution()

alpha = 0.01 # 学习率 alpha
epoch = 500 # 训练全量数据集的轮数

with tf.name_scope('input'):
    # 输入 X，形状[47, 3]
    X = tf.compat.v1.placeholder(tf.float32, X_data.shape, name='X')
    # 输出 y，形状[47, 1]
    y = tf.compat.v1.placeholder(tf.float32, y_data.shape, name='y')

with tf.name_scope('hypothesis'):
    # 权重变量 W，形状[3,1]
    W = tf.compat.v1.get_variable("weights",
                        (X_data.shape[1], 1),
                        initializer=tf.constant_initializer())
    # 假设函数 h(x) = w0*x0+w1*x1+w2*x2, 其中x0恒为1
    # 推理值 y_pred  形状[47,1]
    y_pred = tf.matmul(X, W, name='y_pred')

with tf.name_scope('loss'):
    # 损失函数采用最小二乘法，y_pred - y 是形如[47, 1]的向量。
    # tf.matmul(a,b,transpose_a=True) 表示：矩阵a的转置乘矩阵b，即 [1,47] X [47,1]
    # 损失函数操作 loss
    loss_op = 1 / (2 * len(X_data)) * tf.matmul((y_pred - y), (y_pred - y), transpose_a=True)
with tf.name_scope('train'):
    # 随机梯度下降优化器 opt
    train_op = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=alpha).minimize(loss_op)
    
-----------------------------------------

#训练模型
with tf.compat.v1.Session() as sess:
    # 初始化全局变量
    sess.run(tf.compat.v1.global_variables_initializer())
    # 创建FileWriter实例，并传入当前会话加载的数据流图
    writer = tf.compat.v1.summary.FileWriter('./summary/linear-regression-1', sess.graph)
    # 记录所有损失值
    loss_data = []
    # 开始训练模型
    # 因为训练集较小，所以采用批梯度下降优化算法，每次都使用全量数据训练
    for e in range(1, epoch + 1):
        _, loss, w = sess.run([train_op, loss_op, W], feed_dict={X: X_data, y: y_data})
        # 记录每一轮损失值变化情况
        loss_data.append(float(loss))
        if e % 100 == 0:
            log_str = "Epoch %d \t Loss=%.4g \t Model: y = %.4gx1 + %.4gx2 + %.4g"
            print(log_str % (e, loss, w[1], w[2], w[0]))

# 关闭FileWriter的输出流
writer.close()       

```



