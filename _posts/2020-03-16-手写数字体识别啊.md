---
layout:     post
title:      "手写体数字识别"
subtitle:   "机器学习届中的Hello World，今天给大家整个活"
date:       2020-03-15 
author:     "Luyuan"
header-img: "img/post-bg-unix-linux.jpg"
catalog: true
tags:
    - 机器学习
    - TensorFlow
---


## MNIST手写体数据集介绍

数据集中的图像都是256阶灰度图，即灰度值0表示白色，255表示黑色，使用取值为[0, 255]的uint8数据类型表示图像。为了加速训练需要做数据规范化，将灰度值缩放为[0，1]的float32数据类型。
由于每张图像的尺寸都是28*28像素，我们可以平摊成[784，]的一阶数组，可以表示$256^{784}$张不同的图像。

### 下载和读取MNIST数据集
直接上代码吧
```python
from keras.datasets import mnist
(x_train, y_train),(x_test, y_test) = mnist.load_data(path='absoluteUrl')
```

注意这个API:`tf.kera.datasets.mnist.load_data(path='xxx.npz')`

如果后面不是绝对路径的话，会默认去/root/.kera/路径找path

### 数据集样例可视化

可以借助matplotlib查看
```python
import matplotlib.pyplot as plt

fig = plt.figure()
for i in range(15):
    plt.subplot(3,5,i+1) # 从1位置开始展示，3行5列
    
    plt.tight_layout() # 自适应尺寸
    
    plt.imshow(x_train[i], cmap='Greys')   # 灰色显示像素灰度值 
    
    plt.title('Label: {}'.format(y_train[i])) # 设置标签
    
    plt.xticks([]) # 删除x轴
    
    plt.yticks([]) # 删除Y轴

```

## Softmax网络介绍

在机器学习和认知科学领域，人工神经网络ANN，简称神经网络（NN）是一种模仿生物神经网络的结构和功能的数学模型，用于对函数进行估计或者近似。神经网络为多层神经元的连接，上一层神经元的输出作为下一层神经元的输入。

### 线性不可分

![Linear](https://i.imgur.com/tZmFOLI.jpg)

对于情况1，我们可以很容易地找出一个线性函数，进行二分。
而对于情况2，无法找出线性函数，可以用二次函数等进行划分

为了解决一些线性不可分问题，使用激活函数来引入非线性因素。常用的有Sigmoid/tanh/ReLU等等，如下图。

![激活函数]( )
 
### 全连接层
全连接层是一种对输入数据直接做线性变换的线性计算层。它是神经网络中最常用的一种层，用于学习输出数据和输入数据之间的变化关系。全连接层可作为特征提取层使用，在学习特征的同时实现特征融合；也可以作为最终的分类层使用，其输出神经元的值代表了每个输出类别的概率。 

### 前向传播
首先做一个符号定义：
* L为网络层数，w和b为模型参数，X为输入数据
* $x_i$ 第一层第i个神经元的输入
* $a_i^{(l)}$ 第l层第i个神经元的输出（当l=1时，$a_i^{(1)}=x_i$）
* $w_{ij}^{(l)}$ 第l层第j个神经元到底l+1层第i个神经元的权重
* $b_i^{(l)}$ 第l层第i个神经元的偏置
* $h_{w,b}(X)$ 神经网络（假设函数）输出数据
* $(\vec{a},\vec{b}) = (w^{(l)},b^{(l)}),l=1,...,L$

3层神经网络的计算过程：
$$ 
    a_1^{(2)} = f(w_{11}^{(1)}x_1+w_{12}^{(1)}x_2+w_{13}^{(1)}x_3+b_1^{(1)})
$$
$$
    a_2^{(2)} = f(w_{21}^{(1)}x_1+w_{22}^{(1)}x_2+w_{23}^{(1)}x3+b_2^{(1)})
$$
$$
    a_3^{(2)} = f(w_{31}^{(1)}x_1+w_{32}^{(1)}x_2+w_{33}^{(1)}x3+b_3^{(1)})
$$
$$
    h_{w,b}(X)=a_1^{(3)}   
$$
简化一下： 
$$z_i^{(l)} = \displaystyle \sum^{n}_{j=1}{w_{ij}^{(l-1)}a_j^{(l-1)}+b_i^{(l-1)}}$$
$$a_i^{(l)}=f(z_i^{(l)}),i=1,..,n$$

![](https://i.imgur.com/xdUQg2A.jpg)


### 后向传播

Back Propagation算法的基本思想，是通过损失函数**对模型参数进行求导**，并根据复合函数求导常用的**链式法则**将不同层的模型参数的梯度联系起来，使所有模型参数的梯度更简单。

### MNIST Softmax网络

（终于到了 = =）
将表示手写数字体的一维向量作为输入，中间定义2层512个神经元的隐藏层，具备一定的模型复杂度，足以识别手写体数字；最后定义1层10个神经元的全连接层，用于输出10个不同类别的概率。

![](https://i.imgur.com/G8LK3vp.jpg)

### 代码示例

> 这里有一个Tips，目前看到的教程是Tensorflow1.x的，但是本机安装的是2.0
> 使用这行代码：
> import tensorflow.compat.v1 as tf tf.disable_v2_behavior()

## 卷积神经网络

Convolutional Neural Networks模型是一种以卷积为核心的前馈神经网络模型。

### 卷积
卷积是分析数学中的一种基础运算，其中对输入数据做运算时所用到的函数叫做卷积核。设f(x)和g(x)是R上的两个可积函数，做积分：

$$\int_{-\infty}^{\infty}f(\tau)g(x-\tau)d\tau$$

可以证明，关于所有实数x，上述积分是存在的，这样随着x的不同取值，定义了一个如下的新函数，成为函数f与g的卷积：

$$ 
(f*g)(t) = \int_{R^n}f(\tau)g(t-\tau)d\tau
$$

### 卷积层

Convolution Layer

卷积层是使用一些列卷积核与多通道输入数据做卷积的线性计算层。卷积的提出是为了利用输入数据（如图像）中特征的局域性和位置无关性来降低整个模型的参数量。卷积运算过程与图像处理算法中常用的空间滤波是类似的。因此卷积常被通俗地理解为一种”滤波“过程，卷积核与输入数据作用之后得到了滤波之后的图像，从而提取出了图像的特征。

###  池化层

Pooling

池化层是用于缩小数据规模的一种非线性计算层。为了降低特征维度，我们需要对输入数据进行采样，具体做法是在一个或者多个卷积层后增加一个池化层。池化层由三个参数决定：

* 池化类型：一般有最大池化和平均池化两种
* 池化核的大小k
* 池化核的滑动间隔s

下面举个例子🌰：
![](https://i.imgur.com/5LsDVgk.jpg)

### Dropout层

Dropout是常用的一种正则化方法，Dropout层是一种正则化层。全连接层参数量非常庞大（占据了CNN模型数量的80%~90%左右），发生过拟合问题的风险比较高，所以我们通常需要一些正则化方法训练带有全连接层的CNN模型。在每次迭代训练时，将神经元以一定的概率值暂时丢弃，即在当前迭代中不参与训练。

![](https://i.imgur.com/X8nEAOg.jpg)

### Flatten

字面意思：摊平。将**卷积和池化后的特征** 摊平后输入全连接网络，这里与MNIST softmax网络的输入层类似。MNIST CNN输入特征，MNIST Softmax输入原图。

![](media/15845213672181.jpg)


### 代码示例



 

